{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haar Cascades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the popular [Viola-Jones](https://link.springer.com/article/10.1023/B:VISI.0000013087.49260.fb) framework for object detection. They proposed using Haar filters (two-valued,  rectangular masks), efficiently computed using \"integral\" images (where each pixel value is replaced with the sum of all the pixel values above and to the left of it). Something like a 2D, discrete CDF in probability theory. They also proposed using a \"cascade\" of weak classifiers to improve performance.\n",
    "\n",
    "OpenCV offers an implementation of this under the \"CascadeClassifier\" class, along with pretrained classifiers for face detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces_haar(image_path, output_path): \n",
    "    # Path to preset face detection cascade file that come with OpenCV out of the box. Some other cascade files can be found at venv/Lib/site-packages/cv2/data\n",
    "    face_detection_cascade_file = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "    face_cascade = cv2.CascadeClassifier(face_detection_cascade_file)     \n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    cv2.imshow(\"Base image\", image)\n",
    "    faces_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Parameters for detectMultiScale taken from https://stackoverflow.com/questions/20801015/recommended-values-for-opencv-detectmultiscale-parameters \n",
    "    # along with some experiments on the sample image\n",
    "    faces = face_cascade.detectMultiScale(faces_gray, scaleFactor = 1.2, minNeighbors = 4)      # these values can be changed as needed \n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imwrite(output_path, image)\n",
    "    cv2.imshow(\"Detected faces (Haar Cascade)\", image)\n",
    "    print(\"Close windows or press key to exit.\")\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_from_camera():\n",
    "\n",
    "    face_detection_cascade_file = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "    face_cascade = cv2.CascadeClassifier(face_detection_cascade_file) \n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not access the webcam.\")\n",
    "        return \"Error\"\n",
    "\n",
    "    print(\"Press 'q' to capture an image and 'e' to exit.\")\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame.\")\n",
    "                break\n",
    "\n",
    "            cv2.imshow(\"Camera Feed\", frame)\n",
    "            key = cv2.waitKey(1)\n",
    "            if key == ord(\"q\"):\n",
    "                gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = face_cascade.detectMultiScale(gray_img, scaleFactor = 1.2, minNeighbors = 4)      # these values can be changed as needed\n",
    "\n",
    "                result = \"Non-face\"\n",
    "                color = (0, 0, 255) \n",
    "\n",
    "                if len(faces) > 0:\n",
    "                    result = \"Face\"\n",
    "                    color = (0, 255, 0)\n",
    "                    \n",
    "                    for (x, y, w, h) in faces:\n",
    "                        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    result,\n",
    "                    (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "                cv2.imshow(\"Captured Image\", frame)\n",
    "\n",
    "            if key == ord(\"e\"):\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_faces_haar(\"samples/face_detection/solvay_color.jpg\", \"samples/face_detection/solvay_detected_haar.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_face_from_camera()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works well on the sample image, but the camera feed performance is terrible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to train our own cascade classifier using YT Faces dataset (setup by Ben). Here is the code used to parse the npz files and create a collection of positive examples with annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "count = 0\n",
    "\n",
    "positives_txt_path = os.path.join(\"data\", \"pos.txt\")\n",
    "positives_txt = open(positives_txt_path, \"w\")\n",
    "\n",
    "if not os.path.exists(os.path.join(\"data\",\"positives\")):\n",
    "    print(\"Creating data/positives directory!\")\n",
    "    os.makedirs(\"data\",\"positives\")\n",
    "\n",
    "for folder_idx in [1, 2, 3, 4]:\n",
    "\n",
    "    # You should have these directories after running Ben's setup code \n",
    "    # for yt_faces either using the corresponding flag in installData.py \n",
    "    path = os.path.join(\"data\", \"yt_faces\", f\"youtube_faces_with_keypoints_full_{folder_idx}\", \n",
    "                        f\"youtube_faces_with_keypoints_full_{folder_idx}\")\n",
    "    \n",
    "    with os.scandir(path) as entries:\n",
    "        for file in entries:\n",
    "            # print(file.name)\n",
    "            file_path = os.path.join(path, file.name)\n",
    "            arr = np.load(file_path)\n",
    "            \n",
    "            # Each loaded npz file contains multiple samples (eg, 79 in Aaron_Eckhart_0.npz) in the last dimension.\n",
    "            color_images = arr[\"colorImages\"]    # Shape: (231, 237, 3, 79)\n",
    "            bounding_boxes = arr[\"boundingBox\"]    # Shape: (4, 2, 79)\n",
    "            \n",
    "            num_samples = color_images.shape[-1]\n",
    "            # print(\"Number of samples in file:\", num_samples)\n",
    "\n",
    "            # Iterate over each sample\n",
    "            for sample_idx in range(num_samples):\n",
    "                # Skip 98% of examples since there are ~260k \n",
    "                if random.random() <= 0.98:\n",
    "                    continue \n",
    "                count += 1 \n",
    "                image_frame = color_images[:, :, :, sample_idx]  # (231, 237, 3)\n",
    "                image_frame = cv2.cvtColor(image_frame, cv2.COLOR_RGB2BGR)\n",
    "                bbox = bounding_boxes[:, :, sample_idx]           # (4, 2)\n",
    "                # print(f\"Sample index: {sample_idx} \\nBounding Box:\\n {bbox}\")\n",
    "                \n",
    "                # Resize image to 224x224\n",
    "                original_height, original_width = image_frame.shape[:2]\n",
    "                image_frame = cv2.resize(image_frame, (224, 224))\n",
    "\n",
    "                # Using x,y,w,h here as instructed by the training cascade classifier docs. \n",
    "                x = int(np.min(bbox[:, 0]))\n",
    "                y = int(np.min(bbox[:, 1]))\n",
    "                w = int(np.max(bbox[:, 0]) - x)\n",
    "                h = int(np.max(bbox[:, 1]) - y)\n",
    "\n",
    "                # Rescale bounding box coordinates\n",
    "                x = int(x * (224 / original_width))\n",
    "                y = int(y * (224 / original_height))\n",
    "                w = int(w * (224 / original_width))\n",
    "                h = int(h * (224 / original_height))\n",
    "\n",
    "                # img_filename = f\"img{count}.jpg\"\n",
    "                # img_filepath = os.path.join(\"data\", \"positives\", img_filename)\n",
    "                # cv2.imwrite(img_filepath, image_frame)\n",
    "                \n",
    "                \"\"\" VISUALIZATION ONLY \"\"\"\n",
    "                # cv2.rectangle() expects x1,y1,x2,y2 (top left and bottom right coordinates)\n",
    "                cv2.rectangle(image_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                plt.imshow(cv2.cvtColor(image_frame, cv2.COLOR_BGR2RGB))\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "                # positives_txt.write(f\"data/positives/{img_filename} 1 {x} {y} {w} {h}\\n\")\n",
    "                break\n",
    "            break\n",
    "        \n",
    "positives_txt.close()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = os.path.join(\"data\",\"pos.txt\")\n",
    "output_file = os.path.join(\"data\",\"pos_subset.txt\")\n",
    "\n",
    "with open(input_file, \"r\") as f1:\n",
    "    lines = f1.readlines()\n",
    "\n",
    "# Select random lines to reduce the number of positives, so that we can test if the training even works\n",
    "selected_lines = random.sample(lines, 1500)\n",
    "\n",
    "with open(output_file, \"w\") as f2:\n",
    "    f2.writelines(selected_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create negative examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def pick_random_images_and_write_to_negative_txt(src_dir, dest_dir, num_files=2000):\n",
    "\n",
    "    negatives_txt_path = os.path.join(\"data\", \"neg.txt\")\n",
    "    negatives_txt = open(negatives_txt_path, \"a\")\n",
    "\n",
    "    all_files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n",
    "    \n",
    "    selected_files = random.sample(all_files, min(num_files, len(all_files)))\n",
    "    \n",
    "    for file in selected_files:\n",
    "        shutil.copy(os.path.join(src_dir, file), os.path.join(dest_dir, file))\n",
    "        negatives_txt.write(f\"{dest_dir}\\\\{file}\\n\")   \n",
    "\n",
    "    print(f\"Copied {len(selected_files)} files from {src_dir} to {dest_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives_dir = os.path.join(\"data\",\"negatives\")\n",
    "\"\"\"\n",
    "This data was taken from: https://www.kaggle.com/datasets/mikhailma/house-rooms-streets-image-dataset/data\"\n",
    "Simply download and extract into /data.\n",
    "\n",
    "My reasoning for choosing housing/street data as negative examples was because a) these are higher quality than the CIFAR-10 and caltech negative examples, and b) I think faces are more likely to be seen with these backgrounds so maybe it might help for the negative examples to be indicative of actual backgrounds?\n",
    "\"\"\"\n",
    "kaggle_street_dir = os.path.join(\"data\", \"kaggle_room_street_data\", \"street_data\")\n",
    "kaggle_house_dir = os.path.join(\"data\", \"kaggle_room_street_data\", \"house_data\")\n",
    "\n",
    "if not os.path.exists(negatives_dir):\n",
    "    print(\"Creating data/negatives directory!\")\n",
    "    os.makedirs(negatives_dir)\n",
    "\n",
    "pick_random_images_and_write_to_negative_txt(kaggle_house_dir, negatives_dir)\n",
    "pick_random_images_and_write_to_negative_txt(kaggle_street_dir, negatives_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a vector file using \n",
    "\n",
    "```bash\n",
    "opencv_createsamples.exe -info data/pos_subset.txt -w 64 -h 64 -num 3000 -vec data/pos.vec\n",
    "```\n",
    "\n",
    "This tool is part of a collection of tools that can be installed from https://sourceforge.net/projects/opencvlibrary/files/3.4.16/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should experiment with these \n",
    "ORIENTATIONS = 8\n",
    "PIXELS_PER_CELL = (4,4)\n",
    "CELLS_PER_BLOCK = (2,2)\n",
    "IMAGE_SHAPE = (64,64)     # not 64x128 as in the paper because that was for people detection. We can use square windows for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACE_FOLDER = os.path.join(\"data\", \"face\", \"Face\")\n",
    "NON_FACE_FOLDER = os.path.join(\"data\", \"other\", \"Other\")\n",
    "MAX_IMAGES = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "# These 3 are needed if we didnt use cv2\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from skimage import data, exposure, color\n",
    "\n",
    "# image = asarray(Image.open(\"samples/face_detection/solvay_color.jpg\"))\n",
    "\n",
    "image = cv2.imread(\"samples/face_detection/solvay_color.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "image = cv2.resize(image, IMAGE_SHAPE)  # Resize to 64x128\n",
    "\n",
    "fd, hog_image = hog(\n",
    "    image,\n",
    "    orientations=ORIENTATIONS,\n",
    "    pixels_per_cell=PIXELS_PER_CELL,\n",
    "    cells_per_block=CELLS_PER_BLOCK,\n",
    "    visualize=True,\n",
    "    feature_vector=True\n",
    "    # channel_axis=-1,\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax1.axis('off')\n",
    "ax1.imshow(image, cmap=plt.cm.gray)\n",
    "ax1.set_title('Input image')\n",
    "\n",
    "# Rescale histogram for better display\n",
    "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "ax2.axis('off')\n",
    "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "ax2.set_title('Histogram of Oriented Gradients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(image,\n",
    "                         orientations=ORIENTATIONS,\n",
    "                         pixels_per_cell=PIXELS_PER_CELL,\n",
    "                         cells_per_block=CELLS_PER_BLOCK,\n",
    "                         visualize=False):\n",
    "    \"\"\"\n",
    "    Compute HOG features for a given grayscale image.\n",
    "    The image should be resized to the fixed detection window size.\n",
    "    \"\"\"\n",
    "\n",
    "    image = cv2.resize(image, IMAGE_SHAPE)\n",
    "\n",
    "    if visualize: \n",
    "        features, hog_image = hog(image,\n",
    "                    orientations=orientations,\n",
    "                    pixels_per_cell=pixels_per_cell,\n",
    "                    cells_per_block=cells_per_block,\n",
    "                    block_norm='L2-Hys',\n",
    "                    transform_sqrt=True,\n",
    "                    feature_vector=True,\n",
    "                    visualize=True)\n",
    "        return features, hog_image\n",
    "    \n",
    "    else:\n",
    "        features = hog(image,\n",
    "                    orientations=orientations,\n",
    "                    pixels_per_cell=pixels_per_cell,\n",
    "                    cells_per_block=cells_per_block,\n",
    "                    block_norm='L2-Hys',\n",
    "                    transform_sqrt=True,\n",
    "                    feature_vector=True)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vectors_from_folder(folder, label, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "        Adapted from Ghaith's function in baseline.ipynb \n",
    "\n",
    "        kwargs could contain these keys: \n",
    "            - 'max_images': int value, limiting num images to add to the dataset\n",
    "            - HOG hyperparameters: 'orientations' (int), 'pixels_per_cell', 'cells_per_block' and 'img_size'\n",
    "    \n",
    "        Returns a tuple containing 2 elements: \n",
    "            i)  a list of feature vectors (each feature vector is a list)\n",
    "            ii) a list containing the passed in label repeated max_images times \n",
    "                i.e. a list of all 1's if we choose positive examples folder (i.e. data/face/Face folder) or all 0's for negative examples  \n",
    "    \"\"\"\n",
    "    images_paths = [f for f in os.listdir(folder)]\n",
    "    random.shuffle(images_paths)\n",
    "\n",
    "    # Get kwargs, should just default to constants defined earlier\n",
    "    max_images      = kwargs.get('max_images', None)\n",
    "    orientations    = kwargs.get('orientations', ORIENTATIONS)\n",
    "    pixels_per_cell = kwargs.get('pixels_per_cell', PIXELS_PER_CELL)\n",
    "    cells_per_block = kwargs.get('cells_per_block', CELLS_PER_BLOCK)\n",
    "    img_size        = kwargs.get('img_size', IMAGE_SHAPE)\n",
    "\n",
    "    if max_images is not None:\n",
    "        images_paths = images_paths[:max_images]\n",
    "\n",
    "    features = []\n",
    "    paths = []\n",
    "    for image_path in images_paths:\n",
    "        full_image_path = os.path.join(folder, image_path)\n",
    "        img = cv2.imread(full_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, img_size)\n",
    "            feature_vec = extract_hog_features(img,\n",
    "                                            orientations=orientations,\n",
    "                                            pixels_per_cell=pixels_per_cell,\n",
    "                                            cells_per_block=cells_per_block)\n",
    "            features.append(feature_vec)\n",
    "            paths.append(full_image_path)\n",
    "\n",
    "    return np.array(features), np.full(len(features), label), paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_vectors, face_labels, face_paths = get_feature_vectors_from_folder(\n",
    "    FACE_FOLDER, label=1, max_images=MAX_IMAGES\n",
    ")\n",
    "\n",
    "non_face_vectors, non_face_labels, non_face_paths = get_feature_vectors_from_folder(\n",
    "    NON_FACE_FOLDER, label=0, max_images=len(face_vectors)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took around 40 seconds on my machine for 2000 face and 2000 non-face images (around 100 images/sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.vstack((face_vectors, non_face_vectors))   \n",
    "y = np.hstack((face_labels, non_face_labels))\n",
    "paths = face_paths + non_face_paths\n",
    "\n",
    "# Manually shuffling so we can keep track of the list of image_paths as well\n",
    "shuffled_indices = np.random.permutation(len(X))\n",
    "\n",
    "X = X[shuffled_indices]\n",
    "y = y[shuffled_indices]\n",
    "shuffled_paths = [paths[ind] for ind in shuffled_indices]\n",
    "\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train = X[:split_index]\n",
    "X_test  = X[split_index:]\n",
    "y_train = y[:split_index]\n",
    "y_test  = y[split_index:]\n",
    "shuffled_paths_train = shuffled_paths[:split_index]\n",
    "shuffled_paths_test = shuffled_paths[split_index:]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8100 dimensional feature vectors\n",
    "print(\"Training samples:\", X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(shuffled_paths_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(random_state=42, C = 0.001)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "y_hat = svm.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_hat)}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=np.unique(y_test),\n",
    "    yticklabels=np.unique(y_test),\n",
    ")\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_indices = [i for i in range(len(y_test)) if y_hat[i] != y_test[i]]\n",
    "\n",
    "random.seed(None)\n",
    "random.shuffle(incorrect_indices)\n",
    "random.seed(42)\n",
    "\n",
    "num_to_display = min(len(incorrect_indices), 5)\n",
    "plt.figure(figsize=(15, num_to_display))\n",
    "\n",
    "for i in range(num_to_display):\n",
    "    idx = incorrect_indices[i]\n",
    "\n",
    "    true_label = \"Face\" if y_test[idx] == 1 else \"Non-Face\"\n",
    "    predicted_label = \"Face\" if y_hat[idx] == 1 else \"Non-Face\"\n",
    "    img = cv2.imread(shuffled_paths_test[idx], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    plt.subplot(1, num_to_display, i + 1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(f\"True: {true_label}, Pred: {predicted_label}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image_from_camera():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not access the webcam.\")\n",
    "        return \"Error\"\n",
    "\n",
    "    print(\"Press 'q' to capture an image and 'e' to exit.\")\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame.\")\n",
    "                break\n",
    "\n",
    "            cv2.imshow(\"Camera Feed\", frame)\n",
    "            key = cv2.waitKey(1)\n",
    "            if key == ord(\"q\"):\n",
    "                gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                gray_img = cv2.resize(gray_img, IMAGE_SHAPE)\n",
    "\n",
    "                feature_vec = extract_hog_features(gray_img,\n",
    "                                                orientations=ORIENTATIONS,\n",
    "                                                pixels_per_cell=PIXELS_PER_CELL,\n",
    "                                                cells_per_block=CELLS_PER_BLOCK)\n",
    "\n",
    "                prediction = svm.predict(feature_vec.reshape(1,-1))\n",
    "                result = \"Face\" if prediction == 1 else \"Non-Face\"\n",
    "                color = (0, 255, 0) if prediction == 1 else (0, 0, 255)\n",
    "\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    result,\n",
    "                    (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "                cv2.imshow(\"Captured Image\", gray_img)\n",
    "                cv2.imshow(\"Prediction\", frame)\n",
    "\n",
    "            if key == ord(\"e\"):\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_image_from_camera()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TERRIBLE camera performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Haar cascades on the same test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_cascade_file = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "face_cascade = cv2.CascadeClassifier(face_detection_cascade_file) \n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for idx, image_path in enumerate(shuffled_paths_test):\n",
    "    true_label = y_test[idx]\n",
    "    gray_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    result = 0 # non face by default, unless faces are detected\n",
    "    faces = face_cascade.detectMultiScale(gray_img, scaleFactor = 1.2, minNeighbors = 4)      # these values can be changed as needed\n",
    "    if len(faces) == 1:\n",
    "        result = 1\n",
    "\n",
    "    if result == true_label:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {round((correct/(correct+incorrect)) * 100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
